{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Food Reviews - [Naive Bayes]\n",
    "\n",
    "Data Source:https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "This dataset consists of reviews of fine foods from Amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "![alt text](http://nycdatascience.com/blog/wp-content/uploads/2016/04/NewCover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data includes:\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://nycdatascience.com/blog/wp-content/uploads/2016/04/AmazonReview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Objective:- Review Polarity\n",
    "Given a review, determine the review is positive or negative\n",
    "\n",
    "#### Using text review to decide the polarity\n",
    "Take the summary and text of review and analyze it using NLP whether the customer feedback/review is positive or negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3 as sql\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import random\n",
    "import gensim\n",
    "import warnings\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline \n",
    "# sets the backend of matplotlib to the 'inline' backend:\n",
    "#With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook,\n",
    "#directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document.\n",
    "\n",
    "#Functions to save objects for later use and retireve it\n",
    "import pickle\n",
    "def savetofile(obj,filename):\n",
    "    pickle.dump(obj,open(filename+\".p\",\"wb\"))\n",
    "def openfromfile(filename):\n",
    "    temp = pickle.load(open(filename+\".p\",\"rb\"))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --header=\"Host: e-2106e5ff6b.cognitiveclass.ai\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: _ga=GA1.2.1009651095.1527270727; _xsrf=2|d66eb8d7|8e30b1015ec501038d0632ff567bddb6|1529904261; session=.eJxVj9tugkAURX_FnGdi5FaBxKSgoiZKtYq0Ng0ZYIBRGBQGEY3_XjBt2r6us1f2PjdwjzhPEcWUgcbyEnOASha7LDtgCtoNOg3_gBoJaneSiIVh7x0UXf0wWsTyNNkZorCZKbm5Z6ZAgrAotpbZz3aC4fRn0vyqWFYujKlOp97YWF963kVdE10I-KoKzFLE7OS9rtUzNmRnPlrOlq6a6Updnh00TspafjLEGJ18aSjtfRu4Zo0HGsD9885BgRL2GNiilX18tye70-LNXLw4puw7vLzaWrWdxCN701OH0WAAjVQWOHcJDbPWxCkiSSPnxD_UKCCs-bLP889Ry7t-lgIHIckL5lKU4iaoPzINTdAvnJRH1rJ_mc4PbQu_L39r2i2V55IANEWWROn-BWX4gJ4.Dh-C7A.53fm96PBqDQvenTjy0oa1UWqE_8\" --header=\"Connection: keep-alive\" \"https://e-2106e5ff6b.cognitiveclass.ai/files/Amazon%20Fine%20Food%20Reviews%20Dataset/tfidf_w2v_vec_google.p?download=1\" -O \"tfidf_w2v_vec_google.p\" -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>CleanedText_NoStem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>b'bought sever vital can dog food product foun...</td>\n",
       "      <td>b'bought several vitality canned dog food prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>b'product arriv label jumbo salt peanutsth pea...</td>\n",
       "      <td>b'product arrived labeled jumbo salted peanuts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>b'confect around centuri light pillowi citrus ...</td>\n",
       "      <td>b'confection around centuries light pillowy ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>b'look secret ingredi robitussin believ found ...</td>\n",
       "      <td>b'looking secret ingredient robitussin believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>b'great taffi great price wide assort yummi ta...</td>\n",
       "      <td>b'great taffy great price wide assortment yumm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Id   ProductId          UserId                      ProfileName  \\\n",
       "0      0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1      1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2      2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3      3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4      4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  Positive  1303862400   \n",
       "1                     0                       0  Negative  1346976000   \n",
       "2                     1                       1  Positive  1219017600   \n",
       "3                     3                       3  Negative  1307923200   \n",
       "4                     0                       0  Positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         CleanedText  \\\n",
       "0  b'bought sever vital can dog food product foun...   \n",
       "1  b'product arriv label jumbo salt peanutsth pea...   \n",
       "2  b'confect around centuri light pillowi citrus ...   \n",
       "3  b'look secret ingredi robitussin believ found ...   \n",
       "4  b'great taffi great price wide assort yummi ta...   \n",
       "\n",
       "                                  CleanedText_NoStem  \n",
       "0  b'bought several vitality canned dog food prod...  \n",
       "1  b'product arrived labeled jumbo salted peanuts...  \n",
       "2  b'confection around centuries light pillowy ci...  \n",
       "3  b'looking secret ingredient robitussin believe...  \n",
       "4  b'great taffy great price wide assortment yumm...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using sqlite3 to retrieve data from sqlite file\n",
    "\n",
    "con = sql.connect(\"final.sqlite\")#Loading Cleaned/ Preprocesed text that we did in Text Preprocessing\n",
    "\n",
    "#Using pandas functions to query from sql table\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews \n",
    "\"\"\",con)\n",
    "\n",
    "#Reviews is the name of the table given\n",
    "#Taking only the data where score != 3 as score 3 will be neutral and it won't help us much\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>364171.000000</td>\n",
       "      <td>364171.000000</td>\n",
       "      <td>364171.000000</td>\n",
       "      <td>364171.000000</td>\n",
       "      <td>3.641710e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>241825.377603</td>\n",
       "      <td>261814.561014</td>\n",
       "      <td>1.739021</td>\n",
       "      <td>2.186841</td>\n",
       "      <td>1.296135e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>154519.869452</td>\n",
       "      <td>166958.768333</td>\n",
       "      <td>6.723921</td>\n",
       "      <td>7.348482</td>\n",
       "      <td>4.864772e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.393408e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>104427.500000</td>\n",
       "      <td>113379.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.270858e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>230033.000000</td>\n",
       "      <td>249445.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.311379e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>376763.500000</td>\n",
       "      <td>407408.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.332893e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>525813.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index             Id  HelpfulnessNumerator  \\\n",
       "count  364171.000000  364171.000000         364171.000000   \n",
       "mean   241825.377603  261814.561014              1.739021   \n",
       "std    154519.869452  166958.768333              6.723921   \n",
       "min         0.000000       1.000000              0.000000   \n",
       "25%    104427.500000  113379.500000              0.000000   \n",
       "50%    230033.000000  249445.000000              0.000000   \n",
       "75%    376763.500000  407408.500000              2.000000   \n",
       "max    525813.000000  568454.000000            866.000000   \n",
       "\n",
       "       HelpfulnessDenominator          Time  \n",
       "count           364171.000000  3.641710e+05  \n",
       "mean                 2.186841  1.296135e+09  \n",
       "std                  7.348482  4.864772e+07  \n",
       "min                  0.000000  9.393408e+08  \n",
       "25%                  0.000000  1.270858e+09  \n",
       "50%                  1.000000  1.311379e+09  \n",
       "75%                  2.000000  1.332893e+09  \n",
       "max                878.000000  1.351210e+09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df['Score'].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For EDA and Text Preprecessing Refer other ipynb notebook<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>CleanedText_NoStem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>b'bought sever vital can dog food product foun...</td>\n",
       "      <td>b'bought several vitality canned dog food prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>b'product arriv label jumbo salt peanutsth pea...</td>\n",
       "      <td>b'product arrived labeled jumbo salted peanuts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>b'confect around centuri light pillowi citrus ...</td>\n",
       "      <td>b'confection around centuries light pillowy ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>b'look secret ingredi robitussin believ found ...</td>\n",
       "      <td>b'looking secret ingredient robitussin believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>b'great taffi great price wide assort yummi ta...</td>\n",
       "      <td>b'great taffy great price wide assortment yumm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Id   ProductId          UserId                      ProfileName  \\\n",
       "0      0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1      1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2      2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3      3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4      4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      0  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      0  1219017600   \n",
       "3                     3                       3      1  1307923200   \n",
       "4                     0                       0      0  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         CleanedText  \\\n",
       "0  b'bought sever vital can dog food product foun...   \n",
       "1  b'product arriv label jumbo salt peanutsth pea...   \n",
       "2  b'confect around centuri light pillowi citrus ...   \n",
       "3  b'look secret ingredi robitussin believ found ...   \n",
       "4  b'great taffi great price wide assort yummi ta...   \n",
       "\n",
       "                                  CleanedText_NoStem  \n",
       "0  b'bought several vitality canned dog food prod...  \n",
       "1  b'product arrived labeled jumbo salted peanuts...  \n",
       "2  b'confection around centuries light pillowy ci...  \n",
       "3  b'looking secret ingredient robitussin believe...  \n",
       "4  b'great taffy great price wide assortment yumm...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score as positive/negative -> 0/1\n",
    "def polarity(x):\n",
    "    if x == \"Positive\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df[\"Score\"] = df[\"Score\"].map(polarity) #Map all the scores as the function polarity i.e. positive or negative\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>CleanedText_NoStem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117924</th>\n",
       "      <td>138706</td>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
       "      <td>b'witty little book makes son laugh loud recit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117901</th>\n",
       "      <td>138683</td>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>b'rememb see show air televis year ago child s...</td>\n",
       "      <td>b'remember seeing show aired television years ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298792</th>\n",
       "      <td>417839</td>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>b'beetlejuic well written movi everyth excel a...</td>\n",
       "      <td>b'beetlejuice well written movie everything ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169281</th>\n",
       "      <td>212472</td>\n",
       "      <td>230285</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>b'twist rumplestiskin captur film star michael...</td>\n",
       "      <td>b'twist rumplestiskin captured film starring m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298791</th>\n",
       "      <td>417838</td>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>b'beetlejuic excel funni movi keaton hilari wa...</td>\n",
       "      <td>b'beetlejuice excellent funny movie keaton hil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169342</th>\n",
       "      <td>212533</td>\n",
       "      <td>230348</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "      <td>b'one movi movi collect fill comedi action wha...</td>\n",
       "      <td>b'one movie movie collection filled comedy act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169267</th>\n",
       "      <td>212458</td>\n",
       "      <td>230269</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "      <td>b'always enjoyed movie funny entertaining didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63317</th>\n",
       "      <td>70688</td>\n",
       "      <td>76882</td>\n",
       "      <td>B00002N8SM</td>\n",
       "      <td>A32DW342WBJ6BX</td>\n",
       "      <td>Buttersugar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>948672000</td>\n",
       "      <td>A sure death for flies</td>\n",
       "      <td>I bought a few of these after my apartment was...</td>\n",
       "      <td>b'bought apart infest fruit fli hour trap mani...</td>\n",
       "      <td>b'bought apartment infested fruit flies hours ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169367</th>\n",
       "      <td>212558</td>\n",
       "      <td>230376</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>ACJR7EQF9S6FP</td>\n",
       "      <td>Jeremy Robertson</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>951523200</td>\n",
       "      <td>Bettlejuice...Bettlejuice...BETTLEJUICE!</td>\n",
       "      <td>What happens when you say his name three times...</td>\n",
       "      <td>b'happen say name three time michael keaten st...</td>\n",
       "      <td>b'happens say name three times michael keaten ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169320</th>\n",
       "      <td>212511</td>\n",
       "      <td>230326</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A2DEE7F9XKP3ZR</td>\n",
       "      <td>jerome</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>959990400</td>\n",
       "      <td>Research - Beatlejuice video - French version</td>\n",
       "      <td>I'm getting crazy.I'm looking for Beatlejuice ...</td>\n",
       "      <td>b'get crazyim look beatlejuic french version v...</td>\n",
       "      <td>b'getting crazyim looking beatlejuice french v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index      Id   ProductId          UserId               ProfileName  \\\n",
       "117924  138706  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "117901  138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "298792  417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "169281  212472  230285  B00004RYGX  A344SMIA5JECGM           Vincent P. Ross   \n",
       "298791  417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "169342  212533  230348  B00004RYGX  A1048CYU0OV4O8              Judy L. Eans   \n",
       "169267  212458  230269  B00004RYGX   A1B2IZU1JLZA6                       Wes   \n",
       "63317    70688   76882  B00002N8SM  A32DW342WBJ6BX               Buttersugar   \n",
       "169367  212558  230376  B00004RYGX   ACJR7EQF9S6FP          Jeremy Robertson   \n",
       "169320  212511  230326  B00004RYGX  A2DEE7F9XKP3ZR                    jerome   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "117924                     0                       0      0  939340800   \n",
       "117901                     2                       2      0  940809600   \n",
       "298792                     0                       0      0  944092800   \n",
       "169281                     1                       2      0  944438400   \n",
       "298791                     0                       0      0  946857600   \n",
       "169342                     2                       2      0  947376000   \n",
       "169267                    19                      23      1  948240000   \n",
       "63317                      0                       0      0  948672000   \n",
       "169367                     2                       3      0  951523200   \n",
       "169320                     0                       3      0  959990400   \n",
       "\n",
       "                                                  Summary  \\\n",
       "117924                          EVERY book is educational   \n",
       "117901  This whole series is great way to spend time w...   \n",
       "298792                               Entertainingl Funny!   \n",
       "169281                            A modern day fairy tale   \n",
       "298791                                         FANTASTIC!   \n",
       "169342                                              GREAT   \n",
       "169267    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "63317                              A sure death for flies   \n",
       "169367           Bettlejuice...Bettlejuice...BETTLEJUICE!   \n",
       "169320      Research - Beatlejuice video - French version   \n",
       "\n",
       "                                                     Text  \\\n",
       "117924  this witty little book makes my son laugh at l...   \n",
       "117901  I can remember seeing the show when it aired o...   \n",
       "298792  Beetlejuice is a well written movie ..... ever...   \n",
       "169281  A twist of rumplestiskin captured on film, sta...   \n",
       "298791  Beetlejuice is an excellent and funny movie. K...   \n",
       "169342  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
       "169267  I, myself always enjoyed this movie, it's very...   \n",
       "63317   I bought a few of these after my apartment was...   \n",
       "169367  What happens when you say his name three times...   \n",
       "169320  I'm getting crazy.I'm looking for Beatlejuice ...   \n",
       "\n",
       "                                              CleanedText  \\\n",
       "117924  b'witti littl book make son laugh loud recit c...   \n",
       "117901  b'rememb see show air televis year ago child s...   \n",
       "298792  b'beetlejuic well written movi everyth excel a...   \n",
       "169281  b'twist rumplestiskin captur film star michael...   \n",
       "298791  b'beetlejuic excel funni movi keaton hilari wa...   \n",
       "169342  b'one movi movi collect fill comedi action wha...   \n",
       "169267  b'alway enjoy movi funni entertain didnt hesit...   \n",
       "63317   b'bought apart infest fruit fli hour trap mani...   \n",
       "169367  b'happen say name three time michael keaten st...   \n",
       "169320  b'get crazyim look beatlejuic french version v...   \n",
       "\n",
       "                                       CleanedText_NoStem  \n",
       "117924  b'witty little book makes son laugh loud recit...  \n",
       "117901  b'remember seeing show aired television years ...  \n",
       "298792  b'beetlejuice well written movie everything ex...  \n",
       "169281  b'twist rumplestiskin captured film starring m...  \n",
       "298791  b'beetlejuice excellent funny movie keaton hil...  \n",
       "169342  b'one movie movie collection filled comedy act...  \n",
       "169267  b'always enjoyed movie funny entertaining didn...  \n",
       "63317   b'bought apartment infested fruit flies hours ...  \n",
       "169367  b'happens say name three times michael keaten ...  \n",
       "169320  b'getting crazyim looking beatlejuice french v...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking Whole Data\n",
    "n_samples = 364170\n",
    "df_sample = df\n",
    "\n",
    "###Sorting as we want according to time series\n",
    "df_sample.sort_values('Time',inplace=True) \n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving all samples in disk to as to test to test on the same sample for each of all Algo\n",
    "savetofile(df_sample,\"sample_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening from samples from file\n",
    "df_sample = openfromfile(\"sample_nb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model on Reviews using Different Vectorizing Techniques in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://chrisalbon.com/images/machine_learning_flashcards/Gaussian_Naive_Bayes_Classifier_print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "A commonly used model in methods of Text Classification. As part of the BOW model, a piece of text (sentence or a document) is represented as a bag or multiset of words, disregarding grammar and even word order and the frequency or occurrence of each word is used as a feature for training a classifier.<br>\n",
    "OR <br>\n",
    "Simply,Converting a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 209129)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Text -> Uni gram Vectors\n",
    "uni_gram = CountVectorizer() #in scikit-learn\n",
    "uni_gram_vectors = uni_gram.fit_transform(df_sample['CleanedText'].values)\n",
    "uni_gram_vectors.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#Normalizing the data\n",
    "uni_gram_vectors_norm = preprocessing.normalize(uni_gram_vectors)\n",
    "print(uni_gram_vectors_norm.min())\n",
    "print(uni_gram_vectors_norm.max())\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(uni_gram_vectors_norm,df_sample['Score'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23179, 209129) (23174, 209129)\n",
      "(46353, 209129) (23174, 209129)\n",
      "(69527, 209129) (23174, 209129)\n",
      "(92701, 209129) (23174, 209129)\n",
      "(115875, 209129) (23174, 209129)\n",
      "(139049, 209129) (23174, 209129)\n",
      "(162223, 209129) (23174, 209129)\n",
      "(185397, 209129) (23174, 209129)\n",
      "(208571, 209129) (23174, 209129)\n",
      "(231745, 209129) (23174, 209129)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "for train, cv in tscv.split(X_train):\n",
    "#     print(\"%s %s\" % (train, cv))\n",
    "    print(X_train[train].shape, X_train[cv].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best 'alpha' using Forward Chaining Cross Validation or Time Series CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.39 µs\n",
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n",
      "Best HyperParameter:  {'alpha': 0.001}\n",
      "Best Accuracy: 87.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:   52.3s finished\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "param_grid = {'alpha':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]} #params we need to try on classifier\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(bnb,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>With alpha=0.001 uni_gram has the highest accuracy of 87.45% in Cross Validation<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 87.002%\n",
      "Precision on test set: 0.623\n",
      "Recall on test set: 0.648\n",
      "F1-Score on test set: 0.635\n",
      "Confusion Matrix of test set:\n",
      " [ [TN  FP]\n",
      " [FN TP] ]\n",
      "\n",
      "[[82692  7485]\n",
      " [ 6716 12359]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB(alpha=0.001)\n",
    "bnb.fit(X_train,y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance[Top 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The coef_ attribute of MultinomialNB is a re-parameterization of the naive Bayes model as a linear classifier model. For a binary classification problems this is basically the log of the estimated probability of a feature given the positive class. It means that higher values mean more important features for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tPositive\t\t\t\t\t\tNegative\n",
      "________________________________________________________________________________________________\n",
      "\t-17.4540\taaaa           \t\t\t\t-0.6000\tnot            \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaseri\t\t\t\t-0.9927\ttast           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaargh\t\t\t\t-1.0046\tlike           \n",
      "\t-17.4540\taaaaaaaaagghh  \t\t\t\t-1.2246\tproduct        \n",
      "\t-17.4540\taaaaaaah       \t\t\t\t-1.3641\tone            \n",
      "\t-17.4540\taaaaaaahhhhhh  \t\t\t\t-1.4248\twould          \n",
      "\t-17.4540\taaaaaah        \t\t\t\t-1.4770\ttri            \n",
      "\t-17.4540\taaaaaahhh      \t\t\t\t-1.5611\tflavor         \n",
      "\t-17.4540\taaaaaahhhhh    \t\t\t\t-1.5682\tgood           \n",
      "\t-17.4540\taaaaaahhhhten  \t\t\t\t-1.6188\tbuy            \n",
      "\t-17.4540\taaaaaawwwwwwwwww\t\t\t\t-1.6710\tget            \n",
      "\t-17.4540\taaaaah         \t\t\t\t-1.7154\tuse            \n",
      "\t-17.4540\taaaaahhhhhhhhhhhhhhhhth\t\t\t\t-1.7735\tdont           \n",
      "\t-17.4540\taaaaallll      \t\t\t\t-1.8378\teven           \n",
      "\t-17.4540\taaaaawher      \t\t\t\t-1.8560\torder          \n",
      "\t-17.4540\taaaaawsom      \t\t\t\t-1.9746\tmuch           \n",
      "\t-17.4540\taaaah          \t\t\t\t-1.9772\tmake           \n",
      "\t-17.4540\taaaahhhhhh     \t\t\t\t-2.0297\trealli         \n",
      "\t-17.4540\taaaallll       \t\t\t\t-2.0442\ttime           \n",
      "\t-17.4540\taaaand         \t\t\t\t-2.0579\tlove           \n",
      "\t-17.4540\taaaannnndddgolazo\t\t\t\t-2.0946\tlook           \n",
      "\t-17.4540\taaaarrrrghh    \t\t\t\t-2.1057\tamazon         \n",
      "\t-17.4540\taaagh          \t\t\t\t-2.1092\teat            \n",
      "\t-17.4540\taaah           \t\t\t\t-2.1135\tbought         \n",
      "\t-17.4540\taaahhh         \t\t\t\t-2.1186\tbox            \n"
     ]
    }
   ],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(uni_gram,bnb)\n",
    "#Code Reference:https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 3404647)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#taking one words and two consecutive words together\n",
    "bi_gram = CountVectorizer(ngram_range=(1,2))\n",
    "bi_gram_vectors = bi_gram.fit_transform(df_sample['CleanedText'].values)\n",
    "bi_gram_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "bi_gram_vectors_norm = preprocessing.normalize(bi_gram_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(bi_gram_vectors_norm,df_sample['Score'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23179, 3404647) (23174, 3404647)\n",
      "(46353, 3404647) (23174, 3404647)\n",
      "(69527, 3404647) (23174, 3404647)\n",
      "(92701, 3404647) (23174, 3404647)\n",
      "(115875, 3404647) (23174, 3404647)\n",
      "(139049, 3404647) (23174, 3404647)\n",
      "(162223, 3404647) (23174, 3404647)\n",
      "(185397, 3404647) (23174, 3404647)\n",
      "(208571, 3404647) (23174, 3404647)\n",
      "(231745, 3404647) (23174, 3404647)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "for train, cv in tscv.split(X_train):\n",
    "#     print(\"%s %s\" % (train, cv))\n",
    "    print(X_train[train].shape, X_train[cv].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.39 µs\n",
      "Fitting 10 folds for each of 17 candidates, totalling 170 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 170 out of 170 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HyperParameter:  {'alpha': 0.001}\n",
      "Best Accuracy: 88.96%\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "param_grid = {'alpha':[1000,500,100,50,10,7,6,5,4,2,1,0.5,0.1,0.05,0.01,0.005,0.001]} #params we need to try on classifier\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(bnb,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>With alpha=0.001 bi_gram has the highest accuracy of 88.96% in Cross Validation<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 89.282%\n",
      "Precision on test set: 0.741\n",
      "Recall on test set: 0.594\n",
      "F1-Score on test set: 0.659\n",
      "Confusion Matrix of test set:\n",
      " [ [TN  FP]\n",
      " [FN TP] ]\n",
      "\n",
      "[[86215  3962]\n",
      " [ 7748 11327]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB(alpha=0.001)\n",
    "bnb.fit(X_train,y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance[Top 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tPositive\t\t\t\t\t\tNegative\n",
      "________________________________________________________________________________________________\n",
      "\t-17.4540\taa pleas       \t\t\t\t-0.6000\tnot            \n",
      "\t-17.4540\taa state       \t\t\t\t-0.9927\ttast           \n",
      "\t-17.4540\taaa aaa        \t\t\t\t-1.0046\tlike           \n",
      "\t-17.4540\taaa class      \t\t\t\t-1.2246\tproduct        \n",
      "\t-17.4540\taaa condit     \t\t\t\t-1.3641\tone            \n",
      "\t-17.4540\taaa hockey     \t\t\t\t-1.4248\twould          \n",
      "\t-17.4540\taaa job        \t\t\t\t-1.4770\ttri            \n",
      "\t-17.4540\taaa magazin    \t\t\t\t-1.5611\tflavor         \n",
      "\t-17.4540\taaa perfect    \t\t\t\t-1.5682\tgood           \n",
      "\t-17.4540\taaa plus       \t\t\t\t-1.6188\tbuy            \n",
      "\t-17.4540\taaa rate       \t\t\t\t-1.6710\tget            \n",
      "\t-17.4540\taaa spelt      \t\t\t\t-1.7154\tuse            \n",
      "\t-17.4540\taaa tue        \t\t\t\t-1.7735\tdont           \n",
      "\t-17.4540\taaaa           \t\t\t\t-1.8378\teven           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaseri\t\t\t\t-1.8560\torder          \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaseri good\t\t\t\t-1.9746\tmuch           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaargh\t\t\t\t-1.9772\tmake           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaargh wait\t\t\t\t-2.0297\trealli         \n",
      "\t-17.4540\taaaaaaaaagghh  \t\t\t\t-2.0442\ttime           \n",
      "\t-17.4540\taaaaaaah       \t\t\t\t-2.0579\tlove           \n",
      "\t-17.4540\taaaaaaah good  \t\t\t\t-2.0946\tlook           \n",
      "\t-17.4540\taaaaaaahhhhhh  \t\t\t\t-2.1057\tamazon         \n",
      "\t-17.4540\taaaaaaahhhhhh raspberri\t\t\t\t-2.1092\teat            \n",
      "\t-17.4540\taaaaaah        \t\t\t\t-2.1135\tbought         \n",
      "\t-17.4540\taaaaaah melt   \t\t\t\t-2.1186\tbox            \n"
     ]
    }
   ],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(bi_gram,bnb)\n",
    "#Code Reference:https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11181616/image-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 624 ms, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2)) #Using bi-grams\n",
    "tfidf_vec = tfidf.fit_transform(df_sample['CleanedText'].values)\n",
    "tfidf_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 3404647)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vec_norm = preprocessing.normalize(tfidf_vec)\n",
    "\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vec_norm,df_sample['Score'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n",
      "Fitting 10 folds for each of 17 candidates, totalling 170 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 170 out of 170 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HyperParameter:  {'alpha': 0.001}\n",
      "Best Accuracy: 88.96%\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "param_grid = {'alpha':[1000,500,100,50,10,7,6,5,4,2,1,0.5,0.1,0.05,0.01,0.005,0.001]} #params we need to try on classifier\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(bnb,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>With alpha=0.001 tf-idf has the highest accuracy of 88.96% in Cross Validation<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 89.282%\n",
      "Precision on test set: 0.741\n",
      "Recall on test set: 0.594\n",
      "F1-Score on test set: 0.659\n",
      "Confusion Matrix of test set:\n",
      " [ [TN  FP]\n",
      " [FN TP] ]\n",
      "\n",
      "[[86215  3962]\n",
      " [ 7748 11327]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB(alpha=0.001)\n",
    "bnb.fit(X_train,y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance[Top 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tPositive\t\t\t\t\t\tNegative\n",
      "________________________________________________________________________________________________\n",
      "\t-17.4540\taa pleas       \t\t\t\t-0.6000\tnot            \n",
      "\t-17.4540\taa state       \t\t\t\t-0.9927\ttast           \n",
      "\t-17.4540\taaa aaa        \t\t\t\t-1.0046\tlike           \n",
      "\t-17.4540\taaa class      \t\t\t\t-1.2246\tproduct        \n",
      "\t-17.4540\taaa condit     \t\t\t\t-1.3641\tone            \n",
      "\t-17.4540\taaa hockey     \t\t\t\t-1.4248\twould          \n",
      "\t-17.4540\taaa job        \t\t\t\t-1.4770\ttri            \n",
      "\t-17.4540\taaa magazin    \t\t\t\t-1.5611\tflavor         \n",
      "\t-17.4540\taaa perfect    \t\t\t\t-1.5682\tgood           \n",
      "\t-17.4540\taaa plus       \t\t\t\t-1.6188\tbuy            \n",
      "\t-17.4540\taaa rate       \t\t\t\t-1.6710\tget            \n",
      "\t-17.4540\taaa spelt      \t\t\t\t-1.7154\tuse            \n",
      "\t-17.4540\taaa tue        \t\t\t\t-1.7735\tdont           \n",
      "\t-17.4540\taaaa           \t\t\t\t-1.8378\teven           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaseri\t\t\t\t-1.8560\torder          \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaseri good\t\t\t\t-1.9746\tmuch           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaargh\t\t\t\t-1.9772\tmake           \n",
      "\t-17.4540\taaaaaaaaaaaaaaaaaaaargh wait\t\t\t\t-2.0297\trealli         \n",
      "\t-17.4540\taaaaaaaaagghh  \t\t\t\t-2.0442\ttime           \n",
      "\t-17.4540\taaaaaaah       \t\t\t\t-2.0579\tlove           \n",
      "\t-17.4540\taaaaaaah good  \t\t\t\t-2.0946\tlook           \n",
      "\t-17.4540\taaaaaaahhhhhh  \t\t\t\t-2.1057\tamazon         \n",
      "\t-17.4540\taaaaaaahhhhhh raspberri\t\t\t\t-2.1092\teat            \n",
      "\t-17.4540\taaaaaah        \t\t\t\t-2.1135\tbought         \n",
      "\t-17.4540\taaaaaah melt   \t\t\t\t-2.1186\tbox            \n"
     ]
    }
   ],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(tfidf,bnb)\n",
    "#Code Reference:https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "[Refer Docs] :https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Loading the model from file in the disk\n",
    "w2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vocub = w2vec_model.wv.vocab\n",
    "len(w2v_vocub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Word2Vec\n",
    "* One of the most naive but good ways to convert a sentence into a vector\n",
    "* Convert all the words to vectors and then just take the avg of the vectors the resulting vector represent the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 13s, sys: 548 ms, total: 3min 13s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg_vec_google = [] #List to store all the avg w2vec's \n",
    "# no_datapoints = 364170\n",
    "# sample_cols = random.sample(range(1, no_datapoints), 20001)\n",
    "for sent in df_sample['CleanedText_NoStem']:\n",
    "    cnt = 0 #to count no of words in each reviews\n",
    "    sent_vec = np.zeros(300) #Initializing with zeroes\n",
    "#     print(\"sent:\",sent) \n",
    "    sent = sent.decode(\"utf-8\") \n",
    "    for word in sent.split():\n",
    "        try:\n",
    "#             print(word)\n",
    "            wvec = w2vec_model.wv[word] #Vector of each using w2v model\n",
    "#             print(\"wvec:\",wvec)\n",
    "            sent_vec += wvec #Adding the vectors\n",
    "#             print(\"sent_vec:\",sent_vec)\n",
    "            cnt += 1\n",
    "        except: \n",
    "            pass #When the word is not in the dictionary then do nothing  \n",
    "#     print(sent_vec)\n",
    "    sent_vec /= cnt #Taking average of vectors sum of the particular review\n",
    "#     print(\"avg_vec:\",sent_vec)\n",
    "    avg_vec_google.append(sent_vec) #Storing the avg w2vec's for each review\n",
    "#     print(\"*******************************************************************\")\n",
    "# print(avg_vec_google)\n",
    "avg_vec_google = np.array(avg_vec_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(avg_vec_google).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364167, 300)\n",
      "(364167,)\n"
     ]
    }
   ],
   "source": [
    "mask = ~np.any(np.isnan(avg_vec_google), axis=1)\n",
    "# print(mask)\n",
    "avg_vec_google_new = avg_vec_google[mask]\n",
    "df_sample_new = df_sample['Score'][mask]\n",
    "print(avg_vec_google_new.shape)\n",
    "print(df_sample_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "avg_vec_norm = preprocessing.normalize(avg_vec_google_new)\n",
    "\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(avg_vec_norm,df_sample_new.values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.87 µs\n",
      "Fitting 10 folds for each of 25 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HyperParameter:  {'alpha': 100000}\n",
      "Best Accuracy: 84.68%\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "param_grid = {'alpha':[100000,75000,50000,25000,10000,7500,5000,2500,1000,500,100,50,10,7,6,5,4,2,1,0.5,0.1,0.05,0.01,0.005,0.001]} #params we need to try on classifier\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(bnb,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>With alpha=100000 Avg W2Vec has the highest accuracy of 84.68% in Cross Validation<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 82.533%\n",
      "Precision on test set: 0.250\n",
      "Recall on test set: 0.000\n",
      "F1-Score on test set: 0.000\n",
      "Confusion Matrix of test set:\n",
      " [ [TN  FP]\n",
      " [FN TP] ]\n",
      "\n",
      "[[90164    12]\n",
      " [19071     4]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB(alpha=100000)\n",
    "bnb.fit(X_train,y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance[Top 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tPositive\t\t\t\t\t\tNegative\n",
      "________________________________________________________________________________________________\n",
      "\t-0.8672\tabando         \t\t\t\t-0.5451\tabbazabba      \n",
      "\t-0.8671\taaaaaahhhhh    \t\t\t\t-0.5458\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaseri\n",
      "\t-0.8669\tabomin         \t\t\t\t-0.5463\taagreen        \n",
      "\t-0.8668\tabnormalitiesmi\t\t\t\t-0.5463\taaaaaah        \n",
      "\t-0.8665\taaaaaawwwwwwwwww\t\t\t\t-0.5468\taaah           \n",
      "\t-0.8661\tablsolut       \t\t\t\t-0.5471\taaaaah         \n",
      "\t-0.8647\tabolit         \t\t\t\t-0.5472\tabovea         \n",
      "\t-0.8644\taardvark       \t\t\t\t-0.5475\tabov           \n",
      "\t-0.8641\tabita          \t\t\t\t-0.5476\taaron          \n",
      "\t-0.8631\tabecaus        \t\t\t\t-0.5476\taaaaallll      \n",
      "\t-0.8627\tabouttwin      \t\t\t\t-0.5479\taboutstil      \n",
      "\t-0.8627\taahhh          \t\t\t\t-0.5482\tabor           \n",
      "\t-0.8627\tabondanza      \t\t\t\t-0.5483\taberdeen       \n",
      "\t-0.8624\taboveanim      \t\t\t\t-0.5484\taboutpamela    \n",
      "\t-0.8623\taborio         \t\t\t\t-0.5485\taback          \n",
      "\t-0.8621\taaaaaaahhhhhh  \t\t\t\t-0.5488\tabour          \n",
      "\t-0.8621\taboutamazoncom \t\t\t\t-0.5492\taboutfor       \n",
      "\t-0.8617\taarp           \t\t\t\t-0.5497\taafcoa         \n",
      "\t-0.8616\taboutand       \t\t\t\t-0.5510\tableto         \n",
      "\t-0.8615\tabosout        \t\t\t\t-0.5518\tabovecfh       \n",
      "\t-0.8613\tabbreviatedserv\t\t\t\t-0.5526\taaaaaaah       \n",
      "\t-0.8608\tabnd           \t\t\t\t-0.5527\taboutov        \n",
      "\t-0.8600\tabduct         \t\t\t\t-0.5543\tabouthi        \n",
      "\t-0.8595\tabovedcp       \t\t\t\t-0.5551\tabilityyet     \n",
      "\t-0.8593\tabovehappi     \t\t\t\t-0.5552\tabouteveryth   \n"
     ]
    }
   ],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(uni_gram,bnb)\n",
    "#Code Reference:https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf W2Vec\n",
    "* Another way to covert sentence into vectors\n",
    "* Take weighted sum of the vectors divided by the sum of all the tfidf's \n",
    "<br>i.e. (tfidf(word) x w2v(word))/sum(tfidf's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 589499)\n",
      "CPU times: user 6.15 s, sys: 16 ms, total: 6.16 s\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Taking Sample Data as it was taking more that 10 hours to computer this block\n",
    "n_samples = 25000\n",
    "df_sample_new = df_sample.sample(n_samples)\n",
    "\n",
    "###Sorting as we want according to time series\n",
    "df_sample_new.sort_values('Time',inplace=True) \n",
    "\n",
    "###tf-idf with No Stemming\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2)) #Using bi-grams\n",
    "\n",
    "tfidf_vec_new = tfidf.fit_transform(df_sample_new['CleanedText_NoStem'].values)\n",
    "\n",
    "print(tfidf_vec_new.shape)\n",
    "\n",
    "# tf-idf came up with 2.9 million features for the data corpus\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# tsvd_tfidf_ns = TruncatedSVD(n_components=300)#No of components as total dimensions\n",
    "# tsvd_tfidf_vec_ns = tsvd_tfidf_ns.fit_transform(tfidf_vec_ns)\n",
    "# print(tsvd_tfidf_ns.explained_variance_ratio_[:].sum())\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_w2v_vec_google = []\n",
    "review = 0\n",
    "\n",
    "for sent in df_sample_new['CleanedText_NoStem'].values:\n",
    "    cnt = 0 \n",
    "    weighted_sum  = 0\n",
    "    sent_vec = np.zeros(300)\n",
    "    sent = sent.decode(\"utf-8\") \n",
    "    for word in sent.split():\n",
    "        try:\n",
    "#             print(word)\n",
    "            wvec = w2vec_model.wv[word] #Vector of each using w2v model\n",
    "#             print(\"w2vec:\",wvec)\n",
    "#             print(\"tfidf:\",tfidf_vec_ns[review,features.index(word)])\n",
    "            tfidf_vec = tfidf_vec_new[review,features.index(word)]\n",
    "            sent_vec += (wvec * tfidf_vec)\n",
    "            weighted_sum += tfidf_vec\n",
    "        except:\n",
    "#             print(review)\n",
    "            pass\n",
    "    sent_vec /= weighted_sum\n",
    "#     print(sent_vec)\n",
    "    tfidf_w2v_vec_google.append(sent_vec)\n",
    "    review += 1\n",
    "tfidf_w2v_vec_google = np.array(tfidf_w2v_vec_google)\n",
    "savetofile(tfidf_w2v_vec_google,\"tfidf_w2v_vec_google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precomputed File\n",
    "tfidf_w2v_vec_google = openfromfile(\"tfidf_w2v_vec_google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidfw2v_vecs_norm = preprocessing.normalize(tfidf_w2v_vec_google)\n",
    "\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidfw2v_vecs_norm,df_sample_new['Score'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.58 µs\n",
      "Fitting 10 folds for each of 25 candidates, totalling 250 fits\n",
      "Best HyperParameter:  {'alpha': 5000}\n",
      "Best Accuracy: 84.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:   28.7s finished\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "param_grid = {'alpha':[100000,75000,50000,25000,10000,7500,5000,2500,1000,500,100,50,10,7,6,5,4,2,1,0.5,0.1,0.05,0.01,0.005,0.001]} #params we need to try on classifier\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(bnb,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>With alpha=5000 TF-IDF W2Vec has the highest accuracy of 84.37% in Cross Validation<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 82.320%\n",
      "Precision on test set: 0.000\n",
      "Recall on test set: 0.000\n",
      "F1-Score on test set: 0.000\n",
      "Confusion Matrix of test set:\n",
      " [ [TN  FP]\n",
      " [FN TP] ]\n",
      "\n",
      "[[6174    2]\n",
      " [1324    0]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB(alpha=5000)\n",
    "bnb.fit(X_train,y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions<br>\n",
    "\n",
    "1. The best thing about Naive Bayes it much quicker than algorithms amazingly fast training times\n",
    "2. Best Models are Bi-Gram and Tf-IDF Model with accuracy of 89.28% and precision of 0.741\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
